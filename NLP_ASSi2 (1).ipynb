{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9af5c0a4-b5ad-46eb-8b45-615add036732",
   "metadata": {
    "id": "9af5c0a4-b5ad-46eb-8b45-615add036732"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edcc29c0-d190-4271-aeed-2c18146b70d9",
   "metadata": {
    "id": "edcc29c0-d190-4271-aeed-2c18146b70d9"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\"Spam_Email_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "196e45c1-9b22-4d9b-a568-1b704ce5ee0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "196e45c1-9b22-4d9b-a568-1b704ce5ee0c",
    "outputId": "49ff102a-39aa-4eee-b0ce-98779fa77d48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From gort44@excite.com Mon Jun 24 17:54:21 200...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From fork-admin@xent.com Mon Jul 29 11:39:57 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From dcm123@btamail.net.cn Mon Jun 24 17:49:23...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...       0\n",
       "1  From gort44@excite.com Mon Jun 24 17:54:21 200...       1\n",
       "2  From fork-admin@xent.com Mon Jul 29 11:39:57 2...       1\n",
       "3  From dcm123@btamail.net.cn Mon Jun 24 17:49:23...       1\n",
       "4  From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...       0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b80c8d6e-6da3-4b70-a1cd-6f401dc324c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b80c8d6e-6da3-4b70-a1cd-6f401dc324c4",
    "outputId": "438faa35-6287-4a78-ff45-bc5e426ffb4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5796 entries, 0 to 5795\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5796 non-null   object\n",
      " 1   target  5796 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 90.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display data info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40dd61d3-c28d-4b92-848b-b09ce6324ba9",
   "metadata": {
    "id": "40dd61d3-c28d-4b92-848b-b09ce6324ba9"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #Text text cleansing\n",
    "    # Remove Email header\n",
    "    text =re.sub(r\"^(From|To|Subject|Cc|Bcc|Date|Return-Path|Received|Message-ID):.*?\\n\",\"\",text)\n",
    "\n",
    "    # Remove HTML header\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "     # Remove website URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Remove NoN-alphapectic\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', \"\", text)\n",
    "\n",
    "\n",
    "     # Remove dates\n",
    "    text = re.sub(r\"\\b(?:Sun|Mon|Tue|Wed|Thu|Fri|Sat)\\s(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s(?:[0-9]|[0-2][0-9]|3[0-1])\\s(?:[0-9]{2})?(?:[0-9]{2})?\\s(?:[0-1][0-9]|2[0-3]):(?:[0-5][0-9])(?::(?:[0-5][0-9]))?\\s(?:\\+[0-9]{4}|\\-[0-9]{4}|\\s[A-IK-Z])?\\b\", \"\", text)\n",
    "\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords and stemming\n",
    "    filtered_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    # Joining tokens back into text\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing to the 'Text' column\n",
    "data['text'] = data['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b3f255e-1d89-41c3-9cc7-ec6a7c016644",
   "metadata": {
    "id": "2b3f255e-1d89-41c3-9cc7-ec6a7c016644"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'], test_size=0.4, random_state=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fcb9389-a3d1-40d6-a903-dc6d17ada365",
   "metadata": {
    "id": "0fcb9389-a3d1-40d6-a903-dc6d17ada365"
   },
   "outputs": [],
   "source": [
    "# Model Training (Neural networks based)\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5088ee0-bd8e-4ef3-b1ad-9da5d3f344cd",
   "metadata": {
    "id": "a5088ee0-bd8e-4ef3-b1ad-9da5d3f344cd"
   },
   "outputs": [],
   "source": [
    "# Build Word2Vec model\n",
    "sentences=[row.split() for row in X_train]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f34dd5b5-4ab3-4b48-83aa-bb9ae6f41f7b",
   "metadata": {
    "id": "f34dd5b5-4ab3-4b48-83aa-bb9ae6f41f7b"
   },
   "outputs": [],
   "source": [
    "# Word2Vec embedding for a text\n",
    "def get_word2vec_embedding(text):\n",
    "    words = text.split()\n",
    "    embedding = np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv], axis=0)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bb3d969-f3e8-48b0-95af-077a3a904fdc",
   "metadata": {
    "id": "3bb3d969-f3e8-48b0-95af-077a3a904fdc"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "202c8a02-0dfc-411f-87c6-150cac543f22",
   "metadata": {
    "id": "202c8a02-0dfc-411f-87c6-150cac543f22"
   },
   "outputs": [],
   "source": [
    "# Convert text data into Word2Vec and Doc2Vec embeddings\n",
    "X_train_word2vec = np.array([get_word2vec_embedding(text) for text in X_train])\n",
    "X_test_word2vec = np.array([get_word2vec_embedding(text) for text in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97c771a9-2b88-4d9c-a3e8-a959f250be41",
   "metadata": {
    "id": "97c771a9-2b88-4d9c-a3e8-a959f250be41"
   },
   "outputs": [],
   "source": [
    "# Build Doc2Vec model\n",
    "tagged_documents = [TaggedDocument(words=word_tokenize(text), tags=[i]) for i, text in enumerate(X_train)]\n",
    "doc2vec_model = Doc2Vec(tagged_documents, vector_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b6a01dd-f0b1-47d3-acf7-b424a943b7c9",
   "metadata": {
    "id": "1b6a01dd-f0b1-47d3-acf7-b424a943b7c9"
   },
   "outputs": [],
   "source": [
    "# Doc2Vec embedding for a text\n",
    "def get_doc2vec_embedding(text):\n",
    "    words = text.split()\n",
    "    embedding = doc2vec_model.infer_vector(words)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9aba4f5b-fa16-46a1-8388-ff0a51bed001",
   "metadata": {
    "id": "9aba4f5b-fa16-46a1-8388-ff0a51bed001"
   },
   "outputs": [],
   "source": [
    "# Generate Doc2Vec embeddings for the training data\n",
    "X_train_doc2vec = np.array([get_doc2vec_embedding(text) for text in X_train])\n",
    "\n",
    "# Generate Doc2Vec embeddings for the testing data\n",
    "X_test_doc2vec = np.array([get_doc2vec_embedding(text) for text in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c10c86da-1a78-4a1b-80a1-0f0de892c87c",
   "metadata": {
    "id": "c10c86da-1a78-4a1b-80a1-0f0de892c87c"
   },
   "outputs": [],
   "source": [
    "# Train Decision Tree for word2vec\n",
    "decision_tree_classifier_word2vec = DecisionTreeClassifier()\n",
    "decision_tree_classifier_word2vec.fit(X_train_word2vec, y_train)\n",
    "y_pred_decision_tree_word2vec=decision_tree_classifier_word2vec.predict(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e175140c-8a3d-48c9-8ee7-6308a28ff751",
   "metadata": {
    "id": "e175140c-8a3d-48c9-8ee7-6308a28ff751"
   },
   "outputs": [],
   "source": [
    "# Train Decision Tree for Doc2vec\n",
    "decision_tree_classifier_doc2vec = DecisionTreeClassifier()\n",
    "decision_tree_classifier_doc2vec.fit(X_train_doc2vec, y_train)\n",
    "y_pred_decision_tree_doc2vec=decision_tree_classifier_doc2vec.predict(X_test_doc2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "697e55ae-7fdf-4678-a911-e1eba79da5f5",
   "metadata": {
    "id": "697e55ae-7fdf-4678-a911-e1eba79da5f5"
   },
   "outputs": [],
   "source": [
    "# Train Logistic regression for word2vec\n",
    "logistic_regression_classifier_word2vec = LogisticRegression()\n",
    "logistic_regression_classifier_word2vec.fit(X_train_word2vec, y_train)\n",
    "y_pred_logistic_regression_word2vec=logistic_regression_classifier_word2vec.predict(X_test_word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c7005f0-28cb-4a07-b839-6bcc161ea0f1",
   "metadata": {
    "id": "3c7005f0-28cb-4a07-b839-6bcc161ea0f1"
   },
   "outputs": [],
   "source": [
    "# Train Logistic regression for Dord2vec\n",
    "logistic_regression_classifier_doc2vec = LogisticRegression()\n",
    "logistic_regression_classifier_doc2vec.fit(X_train_doc2vec, y_train)\n",
    "y_pred_logistic_regression_doc2vec=logistic_regression_classifier_doc2vec.predict(X_test_doc2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2k9w6yGKxq-x",
   "metadata": {
    "id": "2k9w6yGKxq-x"
   },
   "outputs": [],
   "source": [
    "#Non nueral text embedding\n",
    "#Bag of word method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the documents into BoW vectors\n",
    "X_BOW = vectorizer.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "BU0K2oicxsPD",
   "metadata": {
    "id": "BU0K2oicxsPD"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train_BOW, X_test_BOW, y_train_BOW, y_test_BOW = train_test_split(X_BOW, data['target'], test_size=0.4, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "xz6yN7ML3sDn",
   "metadata": {
    "id": "xz6yN7ML3sDn"
   },
   "outputs": [],
   "source": [
    "#TF-IDF method\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the documents into TF-IDF vectors\n",
    "X_TFIDF = vectorizer.fit_transform(data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "D6n_cfRB3sui",
   "metadata": {
    "id": "D6n_cfRB3sui"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train_TFIDF, X_test_TFIDF, y_train_TFIDF, y_test_TFIDF = train_test_split(X_TFIDF, data['target'], test_size=0.4, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "rQAkPKepyUUX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQAkPKepyUUX",
    "outputId": "93b06759-3905-4caf-8435-e2a12a4e291c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic regression for BOW\n",
    "logistic_regression_classifier_BOW = LogisticRegression()\n",
    "logistic_regression_classifier_BOW.fit(X_train_BOW, y_train_BOW)\n",
    "y_pred_logistic_regression_BOW=logistic_regression_classifier_BOW.predict(X_test_BOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "yrgSJRef4GRI",
   "metadata": {
    "id": "yrgSJRef4GRI"
   },
   "outputs": [],
   "source": [
    "# Train Logistic regression for TFIDF\n",
    "logistic_regression_classifier_TFIDF = LogisticRegression()\n",
    "logistic_regression_classifier_TFIDF.fit(X_train_TFIDF, y_train_TFIDF)\n",
    "y_pred_logistic_regression_TFIDF=logistic_regression_classifier_TFIDF.predict(X_test_TFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "Q3iuvdj508Eg",
   "metadata": {
    "id": "Q3iuvdj508Eg"
   },
   "outputs": [],
   "source": [
    "# Train Decision Tree for BOW\n",
    "decision_tree_classifier_BOW = DecisionTreeClassifier()\n",
    "decision_tree_classifier_BOW.fit(X_train_BOW, y_train_BOW)\n",
    "y_pred_decision_tree_BOW=decision_tree_classifier_BOW.predict(X_test_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7MIAXyRz4HnP",
   "metadata": {
    "id": "7MIAXyRz4HnP"
   },
   "outputs": [],
   "source": [
    "# Train Decision Tree for TFIDF\n",
    "decision_tree_classifier_TFIDF = DecisionTreeClassifier()\n",
    "decision_tree_classifier_TFIDF.fit(X_train_TFIDF, y_train_TFIDF)\n",
    "y_pred_decision_tree_TFIDF=decision_tree_classifier_TFIDF.predict(X_test_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6fce5ef-f75c-491b-912f-ccffe7b80943",
   "metadata": {
    "id": "e6fce5ef-f75c-491b-912f-ccffe7b80943"
   },
   "outputs": [],
   "source": [
    "result=[]\n",
    "def model_test(model,y_test,y_pred):\n",
    "    accuracy_test=accuracy_score(y_test,y_pred)\n",
    "    precision_test=precision_score(y_test,y_pred)\n",
    "\n",
    "    result.append({\n",
    "        'Model':model,\n",
    "        'Test accuracy': accuracy_test,\n",
    "        'Test precision': precision_test\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d33b5265-33ea-4306-92a5-8c4ab6d0be51",
   "metadata": {
    "id": "d33b5265-33ea-4306-92a5-8c4ab6d0be51"
   },
   "outputs": [],
   "source": [
    "model_test(\"logistic_regression_classifier_word2vec\",y_test,y_pred_logistic_regression_word2vec)\n",
    "model_test(\"y_pred_decision_tree_word2vec\",y_test,y_pred_decision_tree_word2vec)\n",
    "model_test(\"y_pred_logistic_regression_doc2vec\",y_test,y_pred_logistic_regression_doc2vec)\n",
    "model_test(\"decision_tree_classifier_doc2vec\",y_test,y_pred_decision_tree_doc2vec)\n",
    "\n",
    "model_test(\"logistic_regression_classifier_BOW\",y_test_BOW,y_pred_logistic_regression_BOW)\n",
    "model_test(\"decision_tree_classifier_BOW\",y_test_BOW,y_pred_decision_tree_BOW)\n",
    "model_test(\"logistic_regression_classifier_TFIDF\",y_test_TFIDF,y_pred_logistic_regression_TFIDF)\n",
    "model_test(\"decision_tree_classifier_TFIDF\",y_test_TFIDF,y_pred_decision_tree_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdb5bb7e-6933-4ad4-88ed-70f086ff8078",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdb5bb7e-6933-4ad4-88ed-70f086ff8078",
    "outputId": "336ad589-9d26-4622-b72f-6d225312fb8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Results: \n",
      "\n",
      "                                     Model  Test accuracy  Test precision\n",
      "0  logistic_regression_classifier_word2vec       0.984045        0.987552\n",
      "1            y_pred_decision_tree_word2vec       0.976283        0.964817\n",
      "2       y_pred_logistic_regression_doc2vec       0.957309        0.943448\n",
      "3         decision_tree_classifier_doc2vec       0.837861        0.729323\n",
      "4       logistic_regression_classifier_BOW       0.994825        0.994580\n",
      "5             decision_tree_classifier_BOW       0.978439        0.965054\n",
      "6     logistic_regression_classifier_TFIDF       0.982320        0.992968\n",
      "7           decision_tree_classifier_TFIDF       0.982320        0.970470\n"
     ]
    }
   ],
   "source": [
    "models_result=pd.DataFrame(result)\n",
    "print(\"\\nModel Evaluation Results: \\n\")\n",
    "print(models_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c29ee52-b154-43ec-9ff8-a0f9fc10252a",
   "metadata": {
    "id": "6c29ee52-b154-43ec-9ff8-a0f9fc10252a"
   },
   "outputs": [],
   "source": [
    "# Save dataframe to CSV file\n",
    "models_result.to_csv('model_evaluation_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
